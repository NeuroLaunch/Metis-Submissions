{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Natural Language Processing of Economic News Articles_\n",
    "## Post-Metis Analysis - Removal of Headers\n",
    "### (Headers are common text at the beginning of articles, like \"Your Morning News Brief\".)\n",
    "\n",
    "(Jupyter Notebook 1 of 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ------ Section 1:  Load the data as a dataframe and clean up the text -----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, re\n",
    "\n",
    "pd.set_option('max_colwidth',80)\n",
    "\n",
    "DATAPATH = '~/Data/Economic_News'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data set and keep only the important information #\n",
    "filename = os.path.join(os.path.expanduser(DATAPATH), 'NewsEcon2.csv')\n",
    "dfnews = pd.read_csv(filename, encoding ='latin1')\n",
    "dfnews = dfnews[['articleid','headline','text','relevance','positivity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleid</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "      <th>relevance</th>\n",
       "      <th>positivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wsj_398217788</td>\n",
       "      <td>Yields on CDs Fell in the Latest Week</td>\n",
       "      <td>NEW YORK -- Yields on most certificates of deposit offered by major banks dr...</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wsj_399019502</td>\n",
       "      <td>The Morning Brief: White House Seeks to Limit Child Insurance Program</td>\n",
       "      <td>The Wall Street Journal Online&lt;/br&gt;&lt;/br&gt;The Morning Brief, a look at the day...</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       articleid  \\\n",
       "0  wsj_398217788   \n",
       "1  wsj_399019502   \n",
       "\n",
       "                                                                headline  \\\n",
       "0                                  Yields on CDs Fell in the Latest Week   \n",
       "1  The Morning Brief: White House Seeks to Limit Child Insurance Program   \n",
       "\n",
       "                                                                              text  \\\n",
       "0  NEW YORK -- Yields on most certificates of deposit offered by major banks dr...   \n",
       "1  The Wall Street Journal Online</br></br>The Morning Brief, a look at the day...   \n",
       "\n",
       "  relevance  positivity  \n",
       "0       yes         3.0  \n",
       "1        no         NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the first couple of articles #\n",
    "# Note that positivity is NaN when relevance = 'no'.\n",
    "dfnews.head(2)   # size is 8000 rows x 5 colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US relevant article counts = 1420 and 1420.\n"
     ]
    }
   ],
   "source": [
    "# How many articles are a \"yes\" relevance? Trying two methods. #\n",
    "sum1 = dfnews['positivity'].notnull().sum()\n",
    "sum2 = sum(dfnews['relevance']=='yes')\n",
    "\n",
    "print(f'US relevant article counts = {sum1} and {sum2}.')    # 1420, by both methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean documents #\n",
    "def clean_string(document, replace_dict):\n",
    "    # Replace patterns in dict key with dict value that are found in document.\n",
    "    assert type(document)==str\n",
    "    for k,v in replace_dict.items():\n",
    "        document = re.sub(k, v, document)\n",
    "    \n",
    "    return document\n",
    "    \n",
    "REPLACE_TEXT = {r\"</br></br>\" : \". \",  # define non-text characters for replacement\n",
    "                r\"\\'\" : \"\",\n",
    "                r\":\" : \". \",\n",
    "                r\" --\" : \".\", r\"--\" : \".\", r\"-\" : \" \",\n",
    "                r\"a\\.m\\.\" : \"\", r\"p\\.m\\.\" : \"\", r\" am\" : \" \", r\" pm\":  \" \",\n",
    "                r\"A\\.M\\.\" : \"\", r\"P\\.M\\.\" : \"\", r\" AM\" : \" \", r\" PM\":  \" \",\n",
    "                r\"U\\.S\\. \" : \"USA \", r\"U\\.S\\.A\\.\" : \"USA\",\n",
    "                r\"D\\.C\\.\" : \"DC\", r\"J\\.P\\.\" : \"JP\",\n",
    "                r\"Co\\.\" : \"Company\", r\"Corp\\.\" : \"Corporation\",\n",
    "                r\"Jan\\.\" : \"January\", r\"Feb\\.\" : \"February\", r\"Mar\\.\" : \"March\",\n",
    "                r\"Apr\\.\" : \"Apr\", r\"Jun\\.\" : \"June\",\n",
    "                r\"Jul\\.\" : \"July\", r\"Aug\\.\" : \"August\", r\"Sep\\.\" : \"September\",\n",
    "                r\"Oct\\.\" : \"October\", r\"Nov\\.\" : \"November\", r\"Dec\\.\" : \"December\",\n",
    "                r\"%\" : \"\"}\n",
    "\n",
    "REPLACE_NUMBER = {r\"\\d+\\.\\d+\" : \"99\",   # setting to arbitrary number will allow capturing of monetary values\n",
    "                   r\"([^a-zA-Z])(\\d+)\" : r\"\\g<1>99\"}\n",
    "\n",
    "clean_articles = [clean_string(doc, REPLACE_TEXT) for doc in dfnews['text']]\n",
    "clean_articles = [clean_string(doc, REPLACE_NUMBER) for doc in clean_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first several lines (sentences) from a single document #\n",
    "# Acquired text will be assessed for repetitions across documents below.\n",
    "def get_firstlines(document, num_lines, min_char=5, max_char=40):\n",
    "    # Limit text to first 'max_char'\n",
    "    line_list = []\n",
    "    groups = document.split('.')\n",
    "    \n",
    "    for i in range(num_lines):    # store if line exists and is long enough\n",
    "        if len(groups)>=i+1 and (min_char <= len(groups[i]) <= max_char):\n",
    "            text = groups[i]\n",
    "        else:\n",
    "            text = ''\n",
    "        line_list.append(text)\n",
    "    \n",
    "    return tuple(line_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From\n",
    "from scipy import sparse\n",
    "\n",
    "def find_commonlines(lines_ByDoc, verbose=True):\n",
    "    # 'lines_ByDoc': a tuple of lists holding first N lines for each document.\n",
    "    # 'index' of documents to limit search\n",
    "    # Returns a tuple of N ndoc x ndoc matrix of indexes (tuple idx = depth,\n",
    "    #   row = doc w/ common header, col = other doc w/ same header\n",
    "    \n",
    "    ndoc = len(lines_ByDoc)\n",
    "    ndepth = len(lines_ByDoc[0])\n",
    "    headeridx_ByDepth = tuple(sparse.lil_matrix((ndoc,ndoc),dtype=bool) for i in range(ndepth))\n",
    "    \n",
    "    track_str = ''  # for looking at examples of repeated headers\n",
    "    keepind = np.ones((ndoc,),dtype=bool)\n",
    "    \n",
    "    for ld in range(ndepth):\n",
    "        IdxMat = headeridx_ByDepth[ld]  # alias for index matrix for current depth\n",
    "        \n",
    "        for i in range(ndoc):\n",
    "            line1 = lines_ByDoc[i][ld];\n",
    "            if verbose and i%1000==0:  # place here so it doesn't get skipped below\n",
    "                print(i, track_str)\n",
    "                track_str = ''\n",
    "                \n",
    "            if ~keepind[i] or (len(line1)==0):\n",
    "                continue\n",
    "            for k in range(i+1, ndoc):  # only half of documents necessary to analyze\n",
    "                if ~keepind[k]:         # only for depths past the first\n",
    "                    continue\n",
    "                line2 = lines_ByDoc[k][ld];\n",
    "                keep = line1==line2\n",
    "                if keep:\n",
    "                    IdxMat[i,k] = True;\n",
    "                    IdxMat[i,i] = True;\n",
    "                    keepind[k] = False  # skip this entry in subsequent \"i\" and \"k\" searches\n",
    "                    track_str = (i,line1,k,line2)\n",
    "        \n",
    "        nzvec = IdxMat.nonzero()\n",
    "        nzidx = np.unique(np.concatenate((nzvec[0],nzvec[1]),axis=0))\n",
    "        keepind = np.zeros((ndoc,),dtype=bool)\n",
    "        keepind[nzidx] = True           # reset index to only those matching in last round\n",
    "        \n",
    "        if verbose:\n",
    "            print()\n",
    "    \n",
    "    print('Done')\n",
    "    return headeridx_ByDepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "1000 (991, 'THIS WEEK', 3669, 'THIS WEEK')\n",
      "2000 (1888, 'December 99, 99', 2597, 'December 99, 99')\n",
      "3000 (2861, 'SAO PAULO', 3586, 'SAO PAULO')\n",
      "4000 (3307, 'MADRID', 3456, 'MADRID')\n",
      "5000 (4987, 'NEW YORK, January 99 UP)', 6068, 'NEW YORK, January 99 UP)')\n",
      "6000 (5612, 'NEW YORK, June 99 (', 7085, 'NEW YORK, June 99 (')\n",
      "7000 (6861, 'The following is a report of how some major bills fared recently in Congress and a record of how local members of Congress voted', 7934, 'The following is a report of how some major bills fared recently in Congress and a record of how local members of Congress voted')\n",
      "\n",
      "0 (7447, 'ANNAPOLIS', 7688, 'ANNAPOLIS')\n",
      "1000 (922, '  Brett Arends', 1988, '  Brett Arends')\n",
      "2000 (1595, '  patent production', 2036, '  patent production')\n",
      "3000 (2416, '  Jonathan Cheng; Kristina Peterson', 2848, '  Jonathan Cheng; Kristina Peterson')\n",
      "4000 (3246, ' The Federal Reserve Board added 99 stocks to the list of over the counter securities falling under the boards margin, or credit, rules', 3425, ' The Federal Reserve Board added 99 stocks to the list of over the counter securities falling under the boards margin, or credit, rules')\n",
      "5000 (4570, ' This information should not be used as the primary basis of investment decisions', 7258, ' This information should not be used as the primary basis of investment decisions')\n",
      "6000 (5817, ' Sept', 7779, ' Sept')\n",
      "7000 (6581, ' NV means Not Voting', 7934, ' NV means Not Voting')\n",
      "\n",
      "0 (7231, ' PAUL HURD outlines a frightening na ', 7879, ' PAUL HURD outlines a frightening na ')\n",
      "1000 (88, ' Central banks main tools used to be \"open market\" operations, i', 1250, ' Central banks main tools used to be \"open market\" operations, i')\n",
      "2000 (1087, 'than is now presented to the Republican Party', 2164, 'than is now presented to the Republican Party')\n",
      "3000 (2258, ' Meltzer', 3971, ' Meltzer')\n",
      "4000 \n",
      "5000 (4570, ' Stock analysts and brokerage firms may own or actively trade stocks of the companies that they review and recommend', 7258, ' Stock analysts and brokerage firms may own or actively trade stocks of the companies that they review and recommend')\n",
      "6000 \n",
      "7000 \n",
      "\n",
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<8000x8000 sparse matrix of type '<class 'numpy.bool_'>'\n",
       " \twith 1760 stored elements in LInked List format>,\n",
       " <8000x8000 sparse matrix of type '<class 'numpy.bool_'>'\n",
       " \twith 114 stored elements in LInked List format>,\n",
       " <8000x8000 sparse matrix of type '<class 'numpy.bool_'>'\n",
       " \twith 26 stored elements in LInked List format>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at documents w/ first 3 lines that may be similar to others #\n",
    "# Note that leading spaces for lines after the first are retained.\n",
    "first_Lines = [get_firstlines(doc, 3, max_char=150) for doc in clean_articles]\n",
    "commonIdx_ByDepth = find_commonlines(first_Lines, verbose=True)\n",
    "\n",
    "import pickle\n",
    "with open('./saved_files/commonIdx.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(commonIdx_ByDepth, picklefile)\n",
    "\n",
    "commonIdx_ByDepth  # outcome was 1762 articles with a common header at line (depth) 1, 114 at line 2, 26 at line 3;\n",
    "                     # but only 127, 29, and 7 of these are unique! (for a total of 163)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean articles of common headers, based on the find_commonlines() indices; also save the headers #\n",
    "def remove_commonlines(headeridx_ByDepth, lines_ByDoc, docs_in):\n",
    "    ndepth = len(headeridx_ByDepth)\n",
    "    ndocs = len(docs_in)\n",
    "    \n",
    "    assert type(headeridx_ByDepth)==tuple\n",
    "    if headeridx_ByDepth[0].shape[0] != ndocs:\n",
    "        raise ValueError('The index matrices and document list don''t match in length.')\n",
    "\n",
    "    common_dict = {}\n",
    "    docs_out = docs_in.copy()\n",
    "    \n",
    "    for i in range(ndocs):\n",
    "        full_header = ''\n",
    "        code = []\n",
    "        \n",
    "        for ld in range(ndepth):\n",
    "            matchidx = headeridx_ByDepth[ld].getcol(i).nonzero()[0]\n",
    "            if len(matchidx):\n",
    "                matchidx = matchidx[0]          # should be only one element anyway\n",
    "                full_header = full_header + lines_ByDoc[matchidx][ld] + '.'  \n",
    "                code.append(matchidx)           # \".\" necessary because get_firstlines() kept spaces\n",
    "                \n",
    "        docs_out[i] = docs_out[i][len(full_header):]  # remove all of the header lines   \n",
    "        \n",
    "        code = tuple(code)\n",
    "        if len(code) and not common_dict.get(code):\n",
    "            common_dict[code] = full_header\n",
    "            \n",
    "        if i%1000==0:\n",
    "            print(i, end=' ')\n",
    "    \n",
    "    common_headers = [v for k,v in common_dict.items()]\n",
    "    \n",
    "    print('\\nDone')\n",
    "    return docs_out, common_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000 2000 3000 4000 5000 6000 7000 \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# There are 156 unique headers (from 1-3 line depths) #\n",
    "clean_articles2, common_headers = remove_commonlines(commonIdx_ByDepth, first_Lines, clean_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Wall Street Journal Online. The Morning Brief, a look at the days biggest news, is emailed to subscribers by 99  every business day. Sign up for the e mail here.. On Friday evening, with Congress out of town on its summer recess and Americans heading into a mid August weekend, the Bush administration sent a message to the states.  The federal g\n",
      "   --------\n",
      "On Friday evening, with Congress out of town on its summer recess and Americans heading into a mid August weekend, the Bush administration sent a message to the states.  The federal government will make it tougher for a national childrens insurance program to cover the offspring of middle income families. The State Childrens Health Insurance Progra\n"
     ]
    }
   ],
   "source": [
    "# Now clean up a bit more and compare the before/after for one article. Cool! #\n",
    "REPLACE_TEXT = {r\"\\*\\*\\*\" : \"\", r\"\\.[\\s]*\\.\" : \".\", r\",[\\s]*,\" : \",\", r\"^\\s\" : \"\", r\"^\\.\\s*\" : \"\"}\n",
    "clean_articles3 = [clean_string(doc, REPLACE_TEXT) for doc in clean_articles2]\n",
    "\n",
    "print(clean_articles[1][:350])\n",
    "print('   --------')\n",
    "print(clean_articles3[1][:350])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle-save the results #\n",
    "import pickle\n",
    "with open('./saved_files/clean_NewsEcon2_a.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(clean_articles3, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
