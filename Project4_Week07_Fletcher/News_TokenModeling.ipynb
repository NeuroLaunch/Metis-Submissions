{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token modeling\n",
    "(Jupyter Notebook 2 of 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ------ Section 4:  Create topics with latent dirichlet allocation -----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "# nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "import nltk\n",
    "from gensim.models import ldamodel\n",
    "\n",
    "pd.set_option('max_colwidth',80)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NLP_Analysis\n",
    "import importlib\n",
    "importlib.reload(NLP_Analysis)\n",
    "from NLP_Analysis import tokenize, make_dictionary\n",
    "from NLP_Analysis import make_tftable, make_ldatable, classification_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the cleaned up text data and relevance/positivity labels #\n",
    "import pickle\n",
    "\n",
    "with open('data/clean_articles.pkl', 'rb') as picklefile: \n",
    "    Ta = pickle.load(picklefile)\n",
    "with open('data/clean_headlines.pkl', 'rb') as picklefile: \n",
    "    Th = pickle.load(picklefile)\n",
    "with open('data/y_relevance.pkl', 'rb') as picklefile: \n",
    "    y_relevance = pickle.load(picklefile)\n",
    "with open('data/y_positivity.pkl', 'rb') as picklefile: \n",
    "    y_positivity = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets #\n",
    "# Note that the articles (Ta) and headlines (Th) splits are from matching\n",
    "# original documents because 'random_state' arguments are the same. Also,\n",
    "# using relevance in one and positivity in the other allows both sets of\n",
    "# labels to be split. (A rigorous test showed they do match up, row by\n",
    "# row, after the split.)\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "Ta_train, Ta_test, yr_train, yr_test = train_test_split(Ta, y_relevance, \\\n",
    "    test_size=0.25, stratify = y_relevance, random_state=42)\n",
    "Th_train, Th_test, yp_train, yp_test = train_test_split(Th, y_positivity, \\\n",
    "    test_size=0.25, stratify = y_relevance, random_state=42)\n",
    "\n",
    "yr_train[yr_train=='not sure'] = 'no'  # make the labels integers\n",
    "yr_test[yr_test=='not sure'] = 'no'\n",
    "yr_train = [1 if x=='yes' else 0 for x in yr_train]\n",
    "yr_test = [1 if x=='yes' else 0 for x in yr_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500, \n",
      "1000, \n",
      "1500, \n",
      "2000, \n",
      "2500, \n",
      "3000, \n",
      "3500, \n",
      "4000, \n",
      "4500, \n",
      "5000, \n",
      "5500, \n",
      "6000, \n",
      "\n",
      "Dictionary pared from 7827 to 2160 entries.\n",
      "Corpus of 6000 documents created.\n"
     ]
    }
   ],
   "source": [
    "# Create headline corpus for LDA modeling, using only training data #\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "en_stop.add('%')                # percentages are handled separately\n",
    "\n",
    "TAGS = ['NOUN','VERB','ADJ','ADV','ORG']\n",
    "\n",
    "dict_headlines, alltokens = \\\n",
    "    make_dictionary(Th_train, nlp, en_stop, TAGS)\n",
    "dictsize1 = len(dict_headlines)\n",
    "dict_headlines.filter_extremes(no_below=3, no_above=0.8, keep_n=10000)\n",
    "dictsize2 = len(dict_headlines)\n",
    "print(f'Dictionary pared from {dictsize1} to {dictsize2} entries.')\n",
    "\n",
    "corpus_headlines = [dict_headlines.doc2bow(tokens) for tokens in alltokens]\n",
    "print(f'Corpus of {len(corpus_headlines)} documents created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2,\n",
       "  '0.027*\"close\" + 0.027*\"market\" + 0.025*\"seek\" + 0.024*\"back\" + 0.023*\"get\" + 0.021*\"review\" + 0.018*\"buy\" + 0.018*\"outlook\" + 0.016*\"china\" + 0.015*\"investor\"'),\n",
       " (12,\n",
       "  '0.064*\"deficit\" + 0.063*\"trade\" + 0.031*\"news\" + 0.026*\"MONEY\" + 0.020*\"budget\" + 0.018*\"gap\" + 0.016*\"profits\" + 0.016*\"target\" + 0.015*\"keep\" + 0.015*\"export\"')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the LDA model over the corpus. My golly, it actually works. #\n",
    "N_TOPIC = 15\n",
    "lda_headlines = ldamodel.LdaModel(corpus=corpus_headlines,\n",
    "        num_topics=N_TOPIC, minimum_probability=0.01,\n",
    "        id2word=dict_headlines, passes=10)\n",
    "\n",
    "lda_headlines.print_topics(num_topics=2, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.011111111), (1, 0.0111111915), (2, 0.011111111), (3, 0.011111114), (4, 0.011111111), (5, 0.46336082), (6, 0.011111122), (7, 0.011111111), (8, 0.3921946), (9, 0.011111111), (10, 0.011111115), (11, 0.011111111), (12, 0.011111122), (13, 0.011111118), (14, 0.011111113)]\n",
      "Gold Rises Slightly on Bargain Interest\n"
     ]
    }
   ],
   "source": [
    "ldadocs_headlines = [doc for doc in lda_headlines[corpus_headlines]]\n",
    "print(ldadocs_headlines[312])\n",
    "print(Th_train[312])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.027*\"interest\" + 0.026*\"home\" + 0.026*\"sales\" + 0.025*\"see\" + 0.020*\"rise\" + 0.019*\"rates\" + 0.018*\"fear\" + 0.017*\"year\" + 0.015*\"house\" + 0.015*\"signal\" + 0.013*\"gain\" + 0.013*\"new\" + 0.013*\"fall\" + 0.013*\"inflation\" + 0.013*\"lead\"'"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_headlines.print_topic(8, topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500, \n",
      "1000, \n",
      "1500, \n",
      "2000, \n",
      "2500, \n",
      "3000, \n",
      "3500, \n",
      "4000, \n",
      "4500, \n",
      "5000, \n",
      "5500, \n",
      "6000, \n",
      "\n",
      "Dictionary pared from 2160 to 2160 entries.\n",
      "Corpus of 6000 documents created.\n"
     ]
    }
   ],
   "source": [
    "# Repeat for article text training data #\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "en_stop.add('%')                # percentages are handled separately\n",
    "\n",
    "TAGS = ['NOUN','VERB','ADJ','ADV','ORG','GPE']  # add location entities\n",
    "\n",
    "dict_articles, alltokens = \\\n",
    "    make_dictionary(Ta_train, nlp, en_stop, TAGS)\n",
    "dictsize1 = len(dict_articles)\n",
    "dict_articles.filter_extremes(no_below=3, no_above=0.8, keep_n=10000)\n",
    "dictsize2 = len(dict_articles)\n",
    "print(f'Dictionary pared from {dictsize1} to {dictsize2} entries.')\n",
    "\n",
    "corpus_articles = [dict_articles.doc2bow(tokens) for tokens in alltokens]\n",
    "print(f'Corpus of {len(corpus_articles)} documents created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the LDA model over the corpus. My golly, it actually works. #\n",
    "N_TOPIC = 15\n",
    "lda_articles = ldamodel.LdaModel(corpus=corpus_articles,\n",
    "        num_topics=N_TOPIC, minimum_probability=0.01,\n",
    "        id2word=dict_articles, passes=10)\n",
    "\n",
    "lda_articles.print_topics(num_topics=2, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldadocs_articles = [doc for doc in lda_articles[corpus_articles]]\n",
    "# print(ldadocs_articles[812])\n",
    "# print(Ta_train[812])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.016*\"president\" + 0.014*\"say\" + 0.012*\"bush\" + 0.008*\"republican\" + 0.008*\"political\" + 0.007*\"economic\" + 0.007*\"campaign\" + 0.006*\"make\" + 0.006*\"state\" + 0.006*\"election\" + 0.006*\"administration\" + 0.005*\"party\" + 0.005*\"democrat\" + 0.005*\"democratic\" + 0.005*\"get\"'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_articles.print_topic(5, topn=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ------ Section 5:  Create classification and regression models -----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240099</td>\n",
       "      <td>0.138903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.211230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.702103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.531672</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.323883</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.368612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.286489</td>\n",
       "      <td>0.086152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.520998  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.211230  0.000000  0.000000  0.000000   \n",
       "2  0.011111  0.011111  0.011111  0.011111  0.011111  0.011111  0.844444   \n",
       "3  0.011111  0.531672  0.011111  0.011111  0.011111  0.011111  0.011111   \n",
       "4  0.000000  0.000000  0.206367  0.000000  0.000000  0.368612  0.000000   \n",
       "\n",
       "         7         8         9         10        11        12        13  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.240099  0.138903  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.702103  0.000000  0.000000  0.000000   \n",
       "2  0.011111  0.011111  0.011111  0.011111  0.011111  0.011111  0.011111   \n",
       "3  0.011111  0.011111  0.323883  0.011111  0.011111  0.011111  0.011111   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.286489   \n",
       "\n",
       "         14  \n",
       "0  0.000000  \n",
       "1  0.000000  \n",
       "2  0.011111  \n",
       "3  0.011111  \n",
       "4  0.086152  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sparse matrix for input into classifiers and regressors. #\n",
    "# Also create a dataframe version to display.\n",
    "Xh_train = make_ldatable(ldadocs_headlines, N_TOPIC, sparse=True)\n",
    "Xh_train_t = make_ldatable(ldadocs_headlines, N_TOPIC, sparse=False)\n",
    "Xh_train_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.242263</td>\n",
       "      <td>0.040972</td>\n",
       "      <td>0.166047</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.286285</td>\n",
       "      <td>0.115980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096905</td>\n",
       "      <td>0.047282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.586261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.134566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054316</td>\n",
       "      <td>0.216021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.235027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032948</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.082622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.422898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.774289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114242</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.213748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.539009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         1         2         3         4    5         6         7   \\\n",
       "0  0.0  0.000000  0.242263  0.040972  0.166047  0.0  0.000000  0.286285   \n",
       "1  0.0  0.586261  0.000000  0.000000  0.000000  0.0  0.134566  0.000000   \n",
       "2  0.0  0.000000  0.126052  0.000000  0.235027  0.0  0.000000  0.000000   \n",
       "3  0.0  0.000000  0.000000  0.000000  0.000000  0.0  0.000000  0.095469   \n",
       "4  0.0  0.000000  0.000000  0.000000  0.000000  0.0  0.000000  0.000000   \n",
       "\n",
       "         8    9         10        11        12        13        14  \n",
       "0  0.115980  0.0  0.000000  0.000000  0.000000  0.096905  0.047282  \n",
       "1  0.000000  0.0  0.000000  0.054316  0.216021  0.000000  0.000000  \n",
       "2  0.032948  0.0  0.082622  0.000000  0.422898  0.000000  0.094000  \n",
       "3  0.000000  0.0  0.774289  0.000000  0.000000  0.114242  0.000000  \n",
       "4  0.240944  0.0  0.000000  0.000000  0.213748  0.000000  0.539009  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now for articles #\n",
    "Xa_train = make_ldatable(ldadocs_articles, N_TOPIC, sparse=True)\n",
    "Xa_train_t = make_ldatable(ldadocs_articles, N_TOPIC, sparse=False)\n",
    "Xa_train_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, predict classification of \"relevance\" labels #\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=6, p=2,\n",
      "           weights='uniform')\n",
      "{'n_neighbors': 6}\n",
      "0.41084693787517307\n"
     ]
    }
   ],
   "source": [
    "# Find best parameters for KNN model #\n",
    "model = KNeighborsClassifier()\n",
    "grid_values = {'n_neighbors': [1] + list(range(2, 21, 2))}\n",
    "\n",
    "grid_knn = GridSearchCV(model, cv=3, param_grid=grid_values, scoring='precision')\n",
    "grid_knn.fit(Xa_train, yr_train);\n",
    "\n",
    "print(grid_knn.best_estimator_)\n",
    "print(grid_knn.best_params_)\n",
    "print(grid_knn.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neuromac/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/neuromac/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/neuromac/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/neuromac/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/neuromac/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/neuromac/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/neuromac/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/neuromac/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/neuromac/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/neuromac/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/neuromac/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/neuromac/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=42, solver='newton-cg',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "{'C': 10.0, 'penalty': 'l2'}\n",
      "0.36067441330599226\n"
     ]
    }
   ],
   "source": [
    "# Find best parameters for Logistic Regression model #\n",
    "# (Terrible, so move on.)\n",
    "model = LogisticRegression(random_state=42, solver='newton-cg')\n",
    "grid_values = {'penalty': ['l2'],'C': np.logspace(-4, 1, 6)}\n",
    "\n",
    "grid_lr = GridSearchCV(model, cv=3, param_grid=grid_values, scoring='precision')\n",
    "grid_lr.fit(Xa_train, yr_train);\n",
    "\n",
    "print(grid_lr.best_estimator_)\n",
    "print(grid_lr.best_params_)\n",
    "print(grid_lr.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=0.4, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "{'max_features': 0.4, 'n_estimators': 500}\n",
      "0.41357630008525154\n"
     ]
    }
   ],
   "source": [
    "# Find best parameters for Random Forest model #\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "grid_values = {'n_estimators': np.array([100,250,500]),'max_features': np.linspace(0.4,0.8,3)}\n",
    "\n",
    "grid_rf = GridSearchCV(model, cv=3, param_grid=grid_values, scoring='precision')\n",
    "grid_rf.fit(Xa_train, yr_train);\n",
    "\n",
    "print(grid_rf.best_estimator_)\n",
    "print(grid_rf.best_params_)\n",
    "print(grid_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the results of the CV search #\n",
    "cv_test_scores = grid_rf.cv_results_['mean_test_score']\n",
    "cv_params = grid_rf.cv_results_['params']\n",
    "\n",
    "xx_sm, yy_sm = [], []\n",
    "xx_md, yy_md = [], []\n",
    "xx_lg, yy_lg = [], []\n",
    "for p,y in zip(cv_params,cv_test_scores):\n",
    "    if p['n_estimators']==100:\n",
    "        xx_sm.append(p['max_features'])\n",
    "        yy_sm.append(y)\n",
    "    elif p['n_estimators']==250:\n",
    "        xx_md.append(p['max_features'])\n",
    "        yy_md.append(y)\n",
    "    elif p['n_estimators']==500:\n",
    "        xx_lg.append(p['max_features'])\n",
    "        yy_lg.append(y)\n",
    "\n",
    "plt.plot(xx_sm,yy_sm);\n",
    "plt.plot(xx_md,yy_md);\n",
    "plt.plot(xx_lg,yy_lg);\n",
    "plt.xlabel('Max # of Features (percent)', fontsize=12);\n",
    "plt.ylabel('Weighted\\nAccuracy', fontsize=11, rotation=0, ha='right');\n",
    "plt.gca().yaxis.set_label_coords(0.01,1.02)\n",
    "plt.legend(['N = 100','N = 250','N = 500']);\n",
    "\n",
    "# fig = plt.gcf()\n",
    "# fig.savefig('supporting_files/RF_Parameters.svg', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500, \n",
      "1000, \n",
      "1500, \n",
      "2000, \n",
      "\n",
      "[(14, 0.93777764)]\n",
      "Annualized interest rates on certain investments as reported by the Federal Reserve Board on a weekly-average basis  WEEK ENDED  Jun 03, May 27, 2005 2005\n"
     ]
    }
   ],
   "source": [
    "# Fit the LDA model to the test corpus, using the training dictionary #\n",
    "_, alltokens = make_dictionary(Ta_test, nlp, en_stop, TAGS)\n",
    "corpus_articles_test = [dict_articles.doc2bow(tokens) for tokens in alltokens]\n",
    "# lda_articles.fit(corpus_articles_test)\n",
    "\n",
    "ldadocs_articles_test = [doc for doc in lda_articles[corpus_articles_test]]\n",
    "print(ldadocs_articles_test[812])\n",
    "print(Ta_test[812])\n",
    "\n",
    "Xa_test = make_ldatable(ldadocs_articles_test, N_TOPIC, sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.034*\"FED\" + 0.032*\"federal\" + 0.029*\"reserve\" + 0.025*\"fed\" + 0.022*\"bank\" + 0.019*\"rates\" + 0.019*\"interest\" + 0.016*\"banks\" + 0.016*\"say\" + 0.014*\"policy\" + 0.013*\"rate\" + 0.012*\"feed\" + 0.010*\"chairman\" + 0.009*\"inflation\" + 0.009*\"economy\"'"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_articles.print_topic(14, topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Metric ------ Value ----\n",
      " Accuracy:       1.000\n",
      " Precision:      1.000\n",
      " Recall:         1.000\n",
      " F1 Score:       1.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Refit the Random Forest model to the best parameters w/ all data #\n",
    "model = RandomForestClassifier(n_estimators=250, max_features = 0.6,\n",
    "    random_state=42)  # this one overfits, so skip it\n",
    "mode = \n",
    "model.fit(Xa_train, yr_train)\n",
    "ypred = model.predict(Xa_train)\n",
    "\n",
    "classification_score(yr_train, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Metric ------ Value ----\n",
      " Accuracy:       0.803\n",
      " Precision:      0.256\n",
      " Recall:         0.056\n",
      " F1 Score:       0.092\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same as below #\n",
    "ypred_test = grid_knn.predict(Xa_test)\n",
    "classification_score(yr_test, ypred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Metric ------ Value ----\n",
      " Accuracy:       0.803\n",
      " Precision:      0.256\n",
      " Recall:         0.056\n",
      " F1 Score:       0.092\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict the test data set #\n",
    "model = KNeighborsClassifier(n_neighbors=6)\n",
    "model.fit(Xa_train, yr_train);\n",
    "ypred_test = model.predict(Xa_test)\n",
    "\n",
    "classification_score(yr_test, ypred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next up is regression modeling #\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neuromac/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in greater\n",
      "  \n",
      "/Users/neuromac/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in greater\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Make reduced versions of data where 'positivity' is not NaN #\n",
    "keepind = yp_train>-1   # gets around stupid numpy 64\n",
    "XXa_train = Xa_train.toarray()\n",
    "XXa_train = XXa_train[keepind]\n",
    "yyp_train = yp_train[keepind]\n",
    "\n",
    "keepind = yp_test>-1\n",
    "XXa_test = Xa_test.toarray()\n",
    "XXa_test = XXa_test[keepind]\n",
    "yyp_test = yp_test[keepind]\n",
    "\n",
    "XXa_train = np.insert(XXa_train, 0, 1, axis=1)  # add bias term\n",
    "XXa_test = np.insert(XXa_test, 0, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   2.808</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 18 Nov 2018</td> <th>  Prob (F-statistic):</th> <td>0.000263</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:49:11</td>     <th>  Log-Likelihood:    </th> <td> -2042.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1065</td>      <th>  AIC:               </th> <td>   4116.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1049</td>      <th>  BIC:               </th> <td>   4196.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    15</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   27.5261</td> <td>   14.525</td> <td>    1.895</td> <td> 0.058</td> <td>   -0.976</td> <td>   56.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>  -22.8751</td> <td>   14.609</td> <td>   -1.566</td> <td> 0.118</td> <td>  -51.541</td> <td>    5.791</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>  -23.1726</td> <td>   14.621</td> <td>   -1.585</td> <td> 0.113</td> <td>  -51.863</td> <td>    5.518</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>  -23.0948</td> <td>   14.615</td> <td>   -1.580</td> <td> 0.114</td> <td>  -51.772</td> <td>    5.582</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>  -22.2092</td> <td>   14.641</td> <td>   -1.517</td> <td> 0.130</td> <td>  -50.938</td> <td>    6.519</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>  -23.5482</td> <td>   14.561</td> <td>   -1.617</td> <td> 0.106</td> <td>  -52.119</td> <td>    5.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>  -22.1072</td> <td>   14.659</td> <td>   -1.508</td> <td> 0.132</td> <td>  -50.872</td> <td>    6.658</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>  -23.3376</td> <td>   14.612</td> <td>   -1.597</td> <td> 0.111</td> <td>  -52.009</td> <td>    5.334</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>  -22.8073</td> <td>   14.609</td> <td>   -1.561</td> <td> 0.119</td> <td>  -51.474</td> <td>    5.859</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>  -24.9491</td> <td>   14.607</td> <td>   -1.708</td> <td> 0.088</td> <td>  -53.611</td> <td>    3.713</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>  -22.2413</td> <td>   14.690</td> <td>   -1.514</td> <td> 0.130</td> <td>  -51.067</td> <td>    6.584</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>  -21.5390</td> <td>   14.615</td> <td>   -1.474</td> <td> 0.141</td> <td>  -50.216</td> <td>    7.138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>  -22.3900</td> <td>   14.660</td> <td>   -1.527</td> <td> 0.127</td> <td>  -51.157</td> <td>    6.377</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>  -22.3490</td> <td>   14.643</td> <td>   -1.526</td> <td> 0.127</td> <td>  -51.082</td> <td>    6.384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>  -22.1736</td> <td>   14.625</td> <td>   -1.516</td> <td> 0.130</td> <td>  -50.870</td> <td>    6.523</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>  -22.3604</td> <td>   14.608</td> <td>   -1.531</td> <td> 0.126</td> <td>  -51.024</td> <td>    6.303</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>411.477</td> <th>  Durbin-Watson:     </th> <td>   2.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>  55.714</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.093</td>  <th>  Prob(JB):          </th> <td>7.98e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 1.895</td>  <th>  Cond. No.          </th> <td>1.21e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.21e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.039\n",
       "Model:                            OLS   Adj. R-squared:                  0.025\n",
       "Method:                 Least Squares   F-statistic:                     2.808\n",
       "Date:                Sun, 18 Nov 2018   Prob (F-statistic):           0.000263\n",
       "Time:                        23:49:11   Log-Likelihood:                -2042.2\n",
       "No. Observations:                1065   AIC:                             4116.\n",
       "Df Residuals:                    1049   BIC:                             4196.\n",
       "Df Model:                          15                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         27.5261     14.525      1.895      0.058      -0.976      56.028\n",
       "x1           -22.8751     14.609     -1.566      0.118     -51.541       5.791\n",
       "x2           -23.1726     14.621     -1.585      0.113     -51.863       5.518\n",
       "x3           -23.0948     14.615     -1.580      0.114     -51.772       5.582\n",
       "x4           -22.2092     14.641     -1.517      0.130     -50.938       6.519\n",
       "x5           -23.5482     14.561     -1.617      0.106     -52.119       5.023\n",
       "x6           -22.1072     14.659     -1.508      0.132     -50.872       6.658\n",
       "x7           -23.3376     14.612     -1.597      0.111     -52.009       5.334\n",
       "x8           -22.8073     14.609     -1.561      0.119     -51.474       5.859\n",
       "x9           -24.9491     14.607     -1.708      0.088     -53.611       3.713\n",
       "x10          -22.2413     14.690     -1.514      0.130     -51.067       6.584\n",
       "x11          -21.5390     14.615     -1.474      0.141     -50.216       7.138\n",
       "x12          -22.3900     14.660     -1.527      0.127     -51.157       6.377\n",
       "x13          -22.3490     14.643     -1.526      0.127     -51.082       6.384\n",
       "x14          -22.1736     14.625     -1.516      0.130     -50.870       6.523\n",
       "x15          -22.3604     14.608     -1.531      0.126     -51.024       6.303\n",
       "==============================================================================\n",
       "Omnibus:                      411.477   Durbin-Watson:                   2.031\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               55.714\n",
       "Skew:                           0.093   Prob(JB):                     7.98e-13\n",
       "Kurtosis:                       1.895   Cond. No.                     1.21e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.21e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sm.OLS(yyp_train, XXa_train)\n",
    "fit = model.fit()\n",
    "yp_pred = fit.predict(XXa_train)\n",
    "\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.004829950899972735\n"
     ]
    }
   ],
   "source": [
    "# It's terrible. Headlines text won't be better so I'll leave it here. #\n",
    "yp_pred_test = fit.predict(XXa_test)\n",
    "r2 = 1 - (np.sum((yp_pred_test-yyp_test)**2) /\n",
    "    np.sum((yyp_test-yyp_test.mean())**2))\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
