{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Natural Language Processing of Economic News Articles_\n",
    "## Post-Metis Analysis - Token Creation and Visualization\n",
    "\n",
    "(Jupyter Notebook 2 of 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ------ Section 2: Tokenize the cleaned articles -----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from gensim import corpora\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# y_relevance = list(dfnews['relevance'])\n",
    "# y_positivity = list(dfnews['positivity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define typical stop words plus those likely common to economic articles, without impacting positivity #\n",
    "# Note: I shouldn't remove these for bi-gram analysis.\n",
    "STOPWORDS_EXTRA = ['economic', 'finance', 'monetary', 'government', 'bank', 'money', 'amount', 'market']\n",
    "\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "for entry in STOPWORDS_EXTRA:    # could also use spacy's built-in stopword system\n",
    "    en_stop.add(entry)             # (e.g. nlp.vocab['market'].is_stop = True), except doesn't work for _md model\n",
    "    en_stop.add(entry+'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up data version with common headers (e.g. \"NEW YORK --\") removed in NewsClassifier_RemoveHeaders.ipynb #\n",
    "import pickle\n",
    "\n",
    "with open('./saved_files/clean_NewsEcon2_a.pkl', 'rb') as picklefile: \n",
    "    clean_articles = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEPS\n",
    "-- remove stopwords  \n",
    "-- x_bow = bag of words  \n",
    "-- x_tfidf = tf-idf  \n",
    "-- Naive Bayes  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function adapted from original in NLP_Analysis.py #\n",
    "def tokenize(document, nlp, stopwords, stoppos = ['PROPN','DET','PUNCT','NUM'],\n",
    "             NER_tags = ['GRE','LOC','ORG','NORP','PERSON','FAC','LANGUAGE','MONEY','LAW']):\n",
    "    \"\"\" Tokenize items in string 'document', suitable for news articles.\n",
    "        nlp: standard SpaCy object\n",
    "        stopwords, stoppos: list of words (before lemmatizing) and parts-of-speech to exclude\n",
    "    \"\"\"\n",
    "    doc_tokens = []\n",
    "    \n",
    "    # Analyze all words, rejecting unwanted parts of speech #\n",
    "    tokens = nlp(document)  # nlp() = standard SpaCy processing tool\n",
    "    for token in tokens:\n",
    "        if token.is_space or token.is_punct or token.like_url:\n",
    "            continue\n",
    "        if (token.text not in stopwords) and (token.pos_ not in stoppos):\n",
    "            lemma = token.lemma_     # note: this makes the word lower case\n",
    "            if lemma:\n",
    "                doc_tokens.append(lemma)\n",
    "                \n",
    "    for entity in tokens.ents:\n",
    "        if entity.label_ in NER_tags:\n",
    "            doc_tokens.append(entity.text.upper() + ' : ' + entity.label_)\n",
    "            for ent_lemma in nlp(entity.text):\n",
    "                ent_lemma = ent_lemma.lemma_\n",
    "                if doc_tokens.count(ent_lemma):  # remove lemma version if present\n",
    "                    doc_tokens.remove(ent_lemma)\n",
    "\n",
    "    return doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outlook',\n",
       " 'end',\n",
       " 'reading',\n",
       " 'third',\n",
       " 'contract',\n",
       " 'historical',\n",
       " 'minus',\n",
       " 'annualiz',\n",
       " 'would',\n",
       " 'gain',\n",
       " 'enough',\n",
       " 'order',\n",
       " 'G7 GROUP : ORG']"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at a few tokens in an example article #\n",
    "tokens = tokenize(clean_articles[19], nlp, en_stop)\n",
    "tokens[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-208-1267940c92d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcv_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbow_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1032\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    326\u001b[0m                                                tokenize)\n\u001b[1;32m    327\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 328\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Create a bag of words vector for all the documents #\n",
    "test_list = [tokenize(clean_articles[19], nlp, en_stop), tokenize(clean_articles[2], nlp, en_stop)]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv_object = CountVectorizer()\n",
    "bow_array = cv_object.fit_transform(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def my_tokenizer(documents, my_nlp=nlp, my_stopwords=en_stop):\n",
    "#     func = tokenize(documents, nlp = my_nlp, stopwords = my_stopwords)\n",
    "    return tokenize(documents, nlp=my_nlp, stopwords=my_stopwords)\n",
    "\n",
    "cv_object = CountVectorizer(tokenizer=my_tokenizer, preprocessor=None, lowercase=False, analyzer='word')\n",
    "bow_array = cv_object.fit_transform(clean_articles[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$',\n",
       " 'administration',\n",
       " 'children',\n",
       " 'employee',\n",
       " 'important',\n",
       " 'mid',\n",
       " 'political',\n",
       " 'sale',\n",
       " 'there',\n",
       " 'yen']"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_object.get_feature_names()[::50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 1, 4],\n",
       "       [2, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 2, 0, ..., 0, 0, 0],\n",
       "       [2, 0, 2, ..., 3, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_array.toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
